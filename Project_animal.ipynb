{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2443fa05-ffc8-4409-9fc4-2bd06cce9880",
   "metadata": {},
   "source": [
    "# Animal Welfare and Stress Detection System\n",
    "\n",
    "**Course:** Pattern Recognition  \n",
    "**Project Type:** Final Group Project (Part 2)  \n",
    "**Dataset:** Animals-10 (Kaggle)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Team Members & Roles\n",
    "\n",
    "- **Technical Lead:**  \n",
    "  *Name:* Ashwin Shrestha  \n",
    "  *Responsibilities:* System design, model implementation, experimental evaluation, Definition and validation of five (5) publication-ready research questions, final submission on Teams.\n",
    "\n",
    "- **Figures, Tables & Presentation**  \n",
    "  *Name:* Arpan Rai \n",
    "  *Responsibilities:* Designs all figures,Prepares and maintains the presentation slides\n",
    "\n",
    "- **Student 3 – Report & Storytelling:**  \n",
    "  *Name:* Ashwin Shrestha/Arpan Rai \n",
    "  *Responsibilities:*Writing the final report using the provided Overleaf template, ensuring narrative coherence across sections, aligning methodology, results, and discussion.\n",
    "\n",
    "---\n",
    "\n",
    "## Project Overview\n",
    "\n",
    "This project presents a **vision-based animal welfare and stress-risk screening system** built using the Animals-10 dataset.  \n",
    "The system does **not diagnose physiological stress directly**; instead, it infers **visual indicators associated with potential stress or welfare compromise**, such as posture and body compactness, and combines them with species recognition and explainability techniques to support human decision-making.\n",
    "\n",
    "The project addresses five research questions (RQ1–RQ5) through structured experiments, quantitative evaluation, and interpretable outputs.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf10a39-e8f8-4935-857e-ea2aec74882c",
   "metadata": {},
   "source": [
    "## Project Summary and Research Questions\n",
    "\n",
    "### Project Summary\n",
    "\n",
    "Animal welfare assessment is a critical task in domains such as farming, veterinary care, and companion animal monitoring. In many real-world settings, welfare and stress cannot be measured directly and must instead be inferred from **visual behavioral indicators**, including posture, body compactness, and activity-related cues.\n",
    "\n",
    "This project proposes a **vision-based animal welfare and stress-risk screening system** using the Animals-10 image dataset. The system leverages convolutional neural networks for species recognition, simple posture-based visual proxies for potential stress indicators, multimodal feature fusion, robustness testing under uncertainty, and explainable decision-support mechanisms. The goal is to demonstrate how pattern recognition techniques can support **early-stage welfare screening**, rather than clinical diagnosis.\n",
    "\n",
    "---\n",
    "\n",
    "### Research Questions\n",
    "\n",
    "**RQ1:**  \n",
    "*How accurately can a convolutional neural network classify animal species from images in the Animals-10 dataset?*\n",
    "\n",
    "**RQ2:**  \n",
    "*Can posture-related visual features be extracted from images and summarized in a way that is meaningful for animal welfare analysis?*\n",
    "\n",
    "**RQ3:**  \n",
    "*Does fusing posture-based features with species-level appearance probabilities improve stress-risk proxy inference compared to unimodal approaches?*\n",
    "\n",
    "**RQ4:**  \n",
    "*How robust is the proposed system under missing or noisy visual information that resembles real-world monitoring conditions?*\n",
    "\n",
    "**RQ5:**  \n",
    "*Do explainable AI techniques and simple rule-based reasoning improve the interpretability and practical usefulness of welfare and stress-risk predictions for end users?*\n",
    "\n",
    "---\n",
    "\n",
    "> **Note:**  \n",
    "> The Animals-10 dataset does not contain ground-truth stress labels. Therefore, this work focuses on **stress-risk proxies derived from visual posture and appearance cues**, and the results are intended for **screening and decision-support purposes**, not physiological stress diagnosis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "42d1d18d-c69c-4fbe-ace1-0e9bd0461091",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\nitro\\Documents\\animals10_project\n",
      "Output folder: C:\\Users\\nitro\\Documents\\animals10_project\\outputs_part2_animals10\\Figures_Tables\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell C: Setup & Imports\n",
    "# =========================\n",
    "\n",
    "import os, zipfile, subprocess, random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "# Output structure\n",
    "BASE_DIR = Path(\"outputs_part2_animals10\")\n",
    "FIG_ROOT = BASE_DIR / \"Figures_Tables\"\n",
    "\n",
    "for rq in [\"RQ1\",\"RQ2\",\"RQ3\",\"RQ4\",\"RQ5\"]:\n",
    "    (FIG_ROOT / rq).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Working directory:\", Path(\".\").resolve())\n",
    "print(\"Output folder:\", FIG_ROOT.resolve())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26683d9e-ef33-4290-92e5-f851ec3f60f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Animals-10 dataset from Kaggle...\n",
      "Extracted animals10.zip\n",
      "✅ Dataset ready at: C:\\Users\\nitro\\Documents\\animals10_project\\data_animals10\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell D: Dataset Download\n",
    "# =========================\n",
    "\n",
    "# Dataset info\n",
    "KAGGLE_DATASET = \"alessiocorrado99/animals10\"\n",
    "DATA_DIR = Path(\"data_animals10\")\n",
    "\n",
    "# Check kaggle.json\n",
    "kaggle_path = Path.home() / \".kaggle\" / \"kaggle.json\"\n",
    "if not kaggle_path.exists():\n",
    "    raise FileNotFoundError(\n",
    "        \"kaggle.json not found. \"\n",
    "        \"Place kaggle.json in ~/.kaggle/ and restart the kernel.\"\n",
    "    )\n",
    "\n",
    "# Create data directory\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Download dataset if not already present\n",
    "zip_files = list(DATA_DIR.glob(\"*.zip\"))\n",
    "if not zip_files:\n",
    "    print(\"Downloading Animals-10 dataset from Kaggle...\")\n",
    "    subprocess.run(\n",
    "        [\"kaggle\", \"datasets\", \"download\", \"-d\", KAGGLE_DATASET, \"-p\", str(DATA_DIR)],\n",
    "        check=True\n",
    "    )\n",
    "else:\n",
    "    print(\"Dataset zip already exists. Skipping download.\")\n",
    "\n",
    "# Extract dataset\n",
    "for zip_path in DATA_DIR.glob(\"*.zip\"):\n",
    "    with zipfile.ZipFile(zip_path, \"r\") as z:\n",
    "        z.extractall(DATA_DIR)\n",
    "    print(f\"Extracted {zip_path.name}\")\n",
    "\n",
    "print(\"✅ Dataset ready at:\", DATA_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5106f7c8-e1de-415f-bd18-dea17cf4a6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Image root found at:\n",
      "C:\\Users\\nitro\\Documents\\animals10_project\\data_animals10\\raw-img\n",
      "Classes found: ['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
      "Number of classes: 10\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell E: Locate Image Root\n",
    "# =========================\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "def find_image_root(base_dir: Path):\n",
    "   \n",
    "    for p in base_dir.rglob(\"*\"):\n",
    "        if p.is_dir():\n",
    "            subdirs = [d for d in p.iterdir() if d.is_dir()]\n",
    "            if len(subdirs) >= 8:  # Animals-10 has 10 classes\n",
    "                # quick sanity check: folder names are non-empty strings\n",
    "                if all(len(d.name) > 0 for d in subdirs):\n",
    "                    return p\n",
    "    return None\n",
    "\n",
    "IMG_ROOT = find_image_root(DATA_DIR)\n",
    "\n",
    "if IMG_ROOT is None:\n",
    "    raise RuntimeError(\"Could not locate image root directory.\")\n",
    "else:\n",
    "    print(\"✅ Image root found at:\")\n",
    "    print(IMG_ROOT.resolve())\n",
    "\n",
    "tmp_ds = ImageFolder(IMG_ROOT)\n",
    "print(\"Classes found:\", tmp_ds.classes)\n",
    "print(\"Number of classes:\", len(tmp_ds.classes))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aedf05a6-9b45-4c3e-951b-7cd026ae7454",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Using device: cpu\n",
      "Classes: ['cane', 'cavallo', 'elefante', 'farfalla', 'gallina', 'gatto', 'mucca', 'pecora', 'ragno', 'scoiattolo']\n",
      "Total images: 26179\n",
      "Split sizes: 2100 450 450\n",
      "✅ DataLoaders ready:\n",
      "train: 2100 val: 450 test: 450\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Cell F: Transforms + Split + DataLoaders\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "# Force CPU for reliability (your GPU shows sm_120 incompatibility with your torch build)\n",
    "device = torch.device(\"cpu\")\n",
    "print(\"✅ Using device:\", device)\n",
    "\n",
    "# Parameters (CPU-friendly)\n",
    "IMG_SIZE = 128        # 128 is fast; change to 224 if you have time\n",
    "BATCH_SIZE = 32\n",
    "SUBSET_N = 3000       # increase (e.g., 6000) if your laptop is fast\n",
    "\n",
    "# Transforms\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.15, contrast=0.15, saturation=0.15),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "eval_tfms = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "# Base dataset for length/classes\n",
    "base_ds = datasets.ImageFolder(IMG_ROOT, transform=eval_tfms)\n",
    "class_names = base_ds.classes\n",
    "n_classes = len(class_names)\n",
    "N = len(base_ds)\n",
    "\n",
    "print(\"Classes:\", class_names)\n",
    "print(\"Total images:\", N)\n",
    "\n",
    "# Subset indices (CPU speed-up)\n",
    "rng = np.random.default_rng(SEED)\n",
    "subset_idx = rng.choice(N, size=min(SUBSET_N, N), replace=False).tolist()\n",
    "random.shuffle(subset_idx)\n",
    "\n",
    "# Split indices\n",
    "n_total = len(subset_idx)\n",
    "n_train = int(0.70 * n_total)\n",
    "n_val   = int(0.15 * n_total)\n",
    "n_test  = n_total - n_train - n_val\n",
    "\n",
    "train_idx = subset_idx[:n_train]\n",
    "val_idx   = subset_idx[n_train:n_train+n_val]\n",
    "test_idx  = subset_idx[n_train+n_val:]\n",
    "\n",
    "print(\"Split sizes:\", n_train, n_val, n_test)\n",
    "\n",
    "# IMPORTANT: separate datasets so transforms are correct\n",
    "train_full = datasets.ImageFolder(IMG_ROOT, transform=train_tfms)\n",
    "eval_full  = datasets.ImageFolder(IMG_ROOT, transform=eval_tfms)\n",
    "\n",
    "train_ds = Subset(train_full, train_idx)\n",
    "val_ds   = Subset(eval_full, val_idx)\n",
    "test_ds  = Subset(eval_full, test_idx)\n",
    "\n",
    "train_dl = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_dl   = DataLoader(val_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "test_dl  = DataLoader(test_ds, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(\"✅ DataLoaders ready:\")\n",
    "print(\"train:\", len(train_ds), \"val:\", len(val_ds), \"test:\", len(test_ds))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3953208c-a0b2-484b-9b16-f424223d2150",
   "metadata": {},
   "source": [
    "## RQ1: Species recognition baseline (CNN)\n",
    "\n",
    "This section evaluates a transfer-learning CNN model (ResNet-18) for classifying the 10 animal categories in the Animals-10 dataset. Species recognition is a prerequisite for downstream welfare-oriented analysis because visual indicators and posture cues can differ across animal species.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "136efe0e-a657-4789-befa-32116bc971b6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: train acc=0.560 f1=0.488 | val acc=0.784 f1=0.750\n",
      "Epoch 2: train acc=0.796 f1=0.773 | val acc=0.840 f1=0.826\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_loss</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_f1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.391590</td>\n",
       "      <td>0.56000</td>\n",
       "      <td>0.488037</td>\n",
       "      <td>0.744297</td>\n",
       "      <td>0.784444</td>\n",
       "      <td>0.750012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.698279</td>\n",
       "      <td>0.79619</td>\n",
       "      <td>0.772784</td>\n",
       "      <td>0.537972</td>\n",
       "      <td>0.840000</td>\n",
       "      <td>0.826238</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  train_loss  train_acc  train_f1  val_loss   val_acc    val_f1\n",
       "0      1    1.391590    0.56000  0.488037  0.744297  0.784444  0.750012\n",
       "1      2    0.698279    0.79619  0.772784  0.537972  0.840000  0.826238"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ1: Train ResNet18 \n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "model = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
    "model.fc = nn.Linear(model.fc.in_features, n_classes)\n",
    "\n",
    "# Freeze backbone for speed\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "for p in model.fc.parameters():\n",
    "    p.requires_grad = True\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.fc.parameters(), lr=1e-3)\n",
    "\n",
    "def run_epoch(dl, train=True):\n",
    "    model.train(train)\n",
    "    y_true, y_pred = [], []\n",
    "    losses = []\n",
    "\n",
    "    for xb, yb in dl:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "\n",
    "        if train:\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        out = model(xb)\n",
    "        loss = criterion(out, yb)\n",
    "\n",
    "        if train:\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        preds = out.argmax(1)\n",
    "\n",
    "        y_true.extend(yb.detach().cpu().numpy().tolist())\n",
    "        y_pred.extend(preds.detach().cpu().numpy().tolist())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    mf1 = f1_score(y_true, y_pred, average=\"macro\")\n",
    "    return float(np.mean(losses)), float(acc), float(mf1)\n",
    "\n",
    "EPOCHS = 2\n",
    "history = []\n",
    "\n",
    "for ep in range(1, EPOCHS + 1):\n",
    "    tr_loss, tr_acc, tr_f1 = run_epoch(train_dl, train=True)\n",
    "    va_loss, va_acc, va_f1 = run_epoch(val_dl, train=False)\n",
    "\n",
    "    history.append([ep, tr_loss, tr_acc, tr_f1, va_loss, va_acc, va_f1])\n",
    "    print(f\"Epoch {ep}: train acc={tr_acc:.3f} f1={tr_f1:.3f} | val acc={va_acc:.3f} f1={va_f1:.3f}\")\n",
    "\n",
    "hist_df = pd.DataFrame(history, columns=[\n",
    "    \"epoch\",\"train_loss\",\"train_acc\",\"train_f1\",\"val_loss\",\"val_acc\",\"val_f1\"\n",
    "])\n",
    "hist_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "215a7ec1-495c-428f-81df-574a3bb7fb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RQ1 Test Accuracy: 0.8177777777777778\n",
      "RQ1 Test Macro-F1: 0.7829122181754088\n",
      "Confusion matrix total: 450 | diagonal correct: 368\n",
      "✅ Saved RQ1_Fig1.pdf, RQ1_Fig2.pdf, RQ1_Tab1.xlsx, RQ1_Tab2.xlsx in: C:\\Users\\nitro\\Documents\\animals10_project\\outputs_part2_animals10\\Figures_Tables\\RQ1\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ1: Outputs\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "# ---- Evaluate on test set ----\n",
    "model.eval()\n",
    "y_true, y_pred = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for xb, yb in test_dl:\n",
    "        xb = xb.to(device)\n",
    "        out = model(xb)\n",
    "        preds = out.argmax(1).cpu().numpy().tolist()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(yb.numpy().tolist())\n",
    "\n",
    "test_acc = accuracy_score(y_true, y_pred)\n",
    "test_f1  = f1_score(y_true, y_pred, average=\"macro\")\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "print(\"RQ1 Test Accuracy:\", test_acc)\n",
    "print(\"RQ1 Test Macro-F1:\", test_f1)\n",
    "print(\"Confusion matrix total:\", cm.sum(), \"| diagonal correct:\", cm.diagonal().sum())\n",
    "\n",
    "# ---- Figure 1: Training curve ----\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "ax.plot(hist_df[\"epoch\"], hist_df[\"train_acc\"], marker=\"o\", label=\"train_acc\")\n",
    "ax.plot(hist_df[\"epoch\"], hist_df[\"val_acc\"], marker=\"o\", label=\"val_acc\")\n",
    "ax.set_xlabel(\"Epoch\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.set_title(\"RQ1: Training curve (ResNet18 pretrained, frozen backbone)\")\n",
    "ax.set_xticks(hist_df[\"epoch\"].tolist())\n",
    "ax.grid(True, linestyle=\"--\", alpha=0.3)\n",
    "ax.legend()\n",
    "fig.savefig(FIG_ROOT/\"RQ1\"/\"RQ1_Fig1.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Figure 2: Confusion matrix ----\n",
    "fig = plt.figure(figsize=(7,6))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(cm)\n",
    "ax.set_title(\"RQ1: Confusion matrix (test set)\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_xticks(range(n_classes))\n",
    "ax.set_yticks(range(n_classes))\n",
    "ax.set_xticklabels(class_names, rotation=45, ha=\"right\")\n",
    "ax.set_yticklabels(class_names)\n",
    "\n",
    "for (i, j), v in np.ndenumerate(cm):\n",
    "    ax.text(j, i, str(v), ha=\"center\", va=\"center\", fontsize=6)\n",
    "\n",
    "fig.savefig(FIG_ROOT/\"RQ1\"/\"RQ1_Fig2.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Table 1: Summary + training history ----\n",
    "rq1_tab1 = pd.DataFrame([{\n",
    "    \"Model\": \"ResNet18 pretrained (frozen backbone)\",\n",
    "    \"IMG_SIZE\": IMG_SIZE,\n",
    "    \"SUBSET_N\": len(train_ds) + len(val_ds) + len(test_ds),\n",
    "    \"Epochs\": int(hist_df[\"epoch\"].max()),\n",
    "    \"Test_Accuracy\": float(test_acc),\n",
    "    \"Test_MacroF1\": float(test_f1),\n",
    "}])\n",
    "\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ1\"/\"RQ1_Tab1.xlsx\") as w:\n",
    "    rq1_tab1.to_excel(w, index=False, sheet_name=\"Summary\")\n",
    "    hist_df.to_excel(w, index=False, sheet_name=\"TrainingHistory\")\n",
    "\n",
    "# ---- Table 2: Per-class classification report ----\n",
    "rep = classification_report(y_true, y_pred, target_names=class_names, output_dict=True)\n",
    "rq1_tab2 = (pd.DataFrame(rep).T\n",
    "            .rename(columns={\"f1-score\":\"f1\", \"support\":\"support_n\"})\n",
    "            .reset_index()\n",
    "            .rename(columns={\"index\":\"class\"}))\n",
    "\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ1\"/\"RQ1_Tab2.xlsx\") as w:\n",
    "    rq1_tab2.to_excel(w, index=False, sheet_name=\"ClassReport\")\n",
    "\n",
    "print(\"✅ Saved RQ1_Fig1.pdf, RQ1_Fig2.pdf, RQ1_Tab1.xlsx, RQ1_Tab2.xlsx in:\", (FIG_ROOT/\"RQ1\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f46256e6-c447-4058-bfc6-1f4e77dc28eb",
   "metadata": {},
   "source": [
    "## RQ2: Extract posture-related visual proxy features\n",
    "\n",
    "This section extracts simple posture-related **visual proxy features** from animal images (e.g., normalized body height, bounding-box aspect ratio, and a compactness-based crouch score). These features are not ground-truth stress labels, but they serve as interpretable indicators that can support welfare-oriented screening.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aea7612-a228-4888-b73c-2930626b7311",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ feat_df created: (450, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>path</th>\n",
       "      <th>species</th>\n",
       "      <th>norm_h</th>\n",
       "      <th>aspect</th>\n",
       "      <th>crouch_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>data_animals10\\raw-img\\pecora\\OIP-AqQfFyxS3j2x...</td>\n",
       "      <td>pecora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.071429</td>\n",
       "      <td>0.107143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>data_animals10\\raw-img\\mucca\\OIP-jYcEj5NnKXwRy...</td>\n",
       "      <td>mucca</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.829268</td>\n",
       "      <td>0.182927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>data_animals10\\raw-img\\pecora\\OIP-L47z6AcTVbth...</td>\n",
       "      <td>pecora</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.333333</td>\n",
       "      <td>0.133333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>data_animals10\\raw-img\\ragno\\OIP-N9pNu0AMQ6deV...</td>\n",
       "      <td>ragno</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.167315</td>\n",
       "      <td>0.116732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>data_animals10\\raw-img\\cavallo\\OIP-gR_Md9sVfB7...</td>\n",
       "      <td>cavallo</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.803333</td>\n",
       "      <td>0.080333</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                path  species  norm_h  \\\n",
       "0  data_animals10\\raw-img\\pecora\\OIP-AqQfFyxS3j2x...   pecora     1.0   \n",
       "1  data_animals10\\raw-img\\mucca\\OIP-jYcEj5NnKXwRy...    mucca     1.0   \n",
       "2  data_animals10\\raw-img\\pecora\\OIP-L47z6AcTVbth...   pecora     1.0   \n",
       "3  data_animals10\\raw-img\\ragno\\OIP-N9pNu0AMQ6deV...    ragno     1.0   \n",
       "4  data_animals10\\raw-img\\cavallo\\OIP-gR_Md9sVfB7...  cavallo     1.0   \n",
       "\n",
       "     aspect  crouch_score  \n",
       "0  1.071429      0.107143  \n",
       "1  1.829268      0.182927  \n",
       "2  1.333333      0.133333  \n",
       "3  1.167315      0.116732  \n",
       "4  0.803333      0.080333  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ2: Posture proxy feature extraction\n",
    "# =========================\n",
    "\n",
    "from PIL import Image\n",
    "from torch.utils.data import Subset\n",
    "\n",
    "# Robust helper: works for ImageFolder, Subset(ImageFolder), Subset(Subset(...)), etc.\n",
    "def get_path_label(ds, i: int):\n",
    "    \"\"\"\n",
    "    Returns (path, label) for datasets that may be wrapped in one or more Subset objects.\n",
    "    \"\"\"\n",
    "    while isinstance(ds, Subset):\n",
    "        i = ds.indices[i]\n",
    "        ds = ds.dataset\n",
    "    if not hasattr(ds, \"samples\"):\n",
    "        raise TypeError(f\"Expected ImageFolder-like dataset with .samples, got: {type(ds)}\")\n",
    "    path, label = ds.samples[i]\n",
    "    return path, label\n",
    "\n",
    "def posture_features(img_path: str):\n",
    "    \"\"\"\n",
    "    Simple, fast posture proxy from a single image:\n",
    "    - norm_h: approximate foreground height normalized by image height\n",
    "    - aspect: approximate bbox width/height\n",
    "    - crouch_score: heuristic combining low height + compactness\n",
    "    \"\"\"\n",
    "    img = Image.open(img_path).convert(\"L\")\n",
    "    g = (np.array(img).astype(np.float32) / 255.0)\n",
    "\n",
    "    # foreground mask (simple threshold)\n",
    "    mask = (g < 0.85).astype(np.uint8)\n",
    "    ys, xs = np.where(mask > 0)\n",
    "\n",
    "    # fallback for near-empty masks\n",
    "    if len(xs) < 80:\n",
    "        return {\"norm_h\": 1.0, \"aspect\": 1.0, \"crouch_score\": 0.5}\n",
    "\n",
    "    x0, x1 = xs.min(), xs.max()\n",
    "    y0, y1 = ys.min(), ys.max()\n",
    "    bw = float(x1 - x0 + 1)\n",
    "    bh = float(y1 - y0 + 1)\n",
    "    H, W = g.shape\n",
    "\n",
    "    aspect = bw / (bh + 1e-6)\n",
    "    norm_h = bh / (H + 1e-6)\n",
    "\n",
    "    # crouch_score: higher when foreground height is small + bbox is compact\n",
    "    crouch_score = float((1.0 - norm_h) * 0.7 + (np.clip(aspect, 0, 3) / 3.0) * 0.3)\n",
    "\n",
    "    return {\"norm_h\": norm_h, \"aspect\": aspect, \"crouch_score\": crouch_score}\n",
    "\n",
    "# Extract features from a subset of the TEST set for speed\n",
    "FEAT_N = min(1200, len(test_ds))\n",
    "rows = []\n",
    "\n",
    "for i in range(FEAT_N):\n",
    "    path, lab = get_path_label(test_ds, i)\n",
    "    f = posture_features(path)\n",
    "    rows.append({\n",
    "        \"path\": path,\n",
    "        \"species\": class_names[lab],\n",
    "        **f\n",
    "    })\n",
    "\n",
    "feat_df = pd.DataFrame(rows)\n",
    "print(\"✅ feat_df created:\", feat_df.shape)\n",
    "feat_df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92492bc0-3081-4adf-b03c-c0c4009eff40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved RQ2_Fig1.pdf, RQ2_Fig2.pdf, RQ2_Tab1.xlsx, RQ2_Tab2.xlsx in: C:\\Users\\nitro\\Documents\\animals10_project\\outputs_part2_animals10\\Figures_Tables\\RQ2\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ2: Outputs (2 figures + 2 tables)\n",
    "# =========================\n",
    "\n",
    "# ---- Figure 1: Mean crouch score by species ----\n",
    "fig = plt.figure(figsize=(8,5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "mean_crouch = feat_df.groupby(\"species\")[\"crouch_score\"].mean().sort_values()\n",
    "ax.barh(mean_crouch.index, mean_crouch.values)\n",
    "ax.set_xlabel(\"Mean crouch score (proxy)\")\n",
    "ax.set_title(\"RQ2: Mean posture-based crouch score by species\")\n",
    "\n",
    "fig.savefig(FIG_ROOT/\"RQ2\"/\"RQ2_Fig1.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Figure 2: Aspect ratio vs normalized height scatter ----\n",
    "fig = plt.figure(figsize=(7,5))\n",
    "ax = fig.add_subplot(111)\n",
    "\n",
    "for sp in sorted(feat_df[\"species\"].unique()):\n",
    "    tmp = feat_df[feat_df[\"species\"] == sp]\n",
    "    ax.scatter(tmp[\"aspect\"], tmp[\"norm_h\"], s=10, alpha=0.35, label=sp)\n",
    "\n",
    "ax.set_xlabel(\"Aspect ratio (bbox width / height)\")\n",
    "ax.set_ylabel(\"Normalized height (bbox height / image height)\")\n",
    "ax.set_title(\"RQ2: Posture proxy feature space (subset)\")\n",
    "ax.legend(fontsize=7, ncols=2)\n",
    "\n",
    "fig.savefig(FIG_ROOT/\"RQ2\"/\"RQ2_Fig2.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Table 1: Mean posture features by species ----\n",
    "rq2_tab1 = (feat_df\n",
    "            .groupby(\"species\")[[\"norm_h\",\"aspect\",\"crouch_score\"]]\n",
    "            .mean()\n",
    "            .reset_index())\n",
    "\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ2\"/\"RQ2_Tab1.xlsx\") as w:\n",
    "    rq2_tab1.to_excel(w, index=False, sheet_name=\"Mean_Features\")\n",
    "\n",
    "# ---- Table 2: Descriptive statistics (mean/std/median) ----\n",
    "rq2_tab2 = (feat_df\n",
    "            .groupby(\"species\")[[\"norm_h\",\"aspect\",\"crouch_score\"]]\n",
    "            .agg([\"mean\",\"std\",\"median\"])\n",
    "            .reset_index())\n",
    "\n",
    "# Flatten column names\n",
    "rq2_tab2.columns = [\"species\"] + [f\"{a}_{b}\" for a,b in rq2_tab2.columns[1:]]\n",
    "\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ2\"/\"RQ2_Tab2.xlsx\") as w:\n",
    "    rq2_tab2.to_excel(w, index=False, sheet_name=\"Descriptive_Stats\")\n",
    "\n",
    "print(\"✅ Saved RQ2_Fig1.pdf, RQ2_Fig2.pdf, RQ2_Tab1.xlsx, RQ2_Tab2.xlsx in:\",\n",
    "      (FIG_ROOT/\"RQ2\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17e1d8d-4154-438c-b9b3-2521969b09de",
   "metadata": {},
   "source": [
    "## RQ3: Does fusing posture and appearance improve stress-risk proxy inference?\n",
    "\n",
    "The Animals-10 dataset does not provide ground-truth stress labels. Therefore, this section constructs a **stress-risk proxy** using posture-based indicators (crouch score) within each species. We compare three models:\n",
    "\n",
    "1) **Posture-only** (visual proxy features)  \n",
    "2) **Appearance-only** (CNN class probability vector)  \n",
    "3) **Fused** (posture + CNN probabilities)\n",
    "\n",
    "We evaluate each model using Accuracy, F1 score, and ROC-AUC on the same train/test split.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bdcc00ff-ab1d-4f41-ab72-366d95a3201a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ meta_df created: (450, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Posture-only</td>\n",
       "      <td>0.902655</td>\n",
       "      <td>0.867470</td>\n",
       "      <td>0.972260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Appearance-only (CNN probs)</td>\n",
       "      <td>0.646018</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.552055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Fused (posture + CNN probs)</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Model  Accuracy        F1   ROC-AUC\n",
       "0                 Posture-only  0.902655  0.867470  0.972260\n",
       "1  Appearance-only (CNN probs)  0.646018  0.047619  0.552055\n",
       "2  Fused (posture + CNN probs)  0.946903  0.926829  0.991781"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ3: Meta-feature dataset + unimodal vs fused models\n",
    "# =========================\n",
    "\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Inference transform (self-contained; avoids NameError)\n",
    "infer_tfm = transforms.Compose([\n",
    "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229,0.224,0.225]),\n",
    "])\n",
    "\n",
    "def cnn_probs(img_path: str):\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    x = infer_tfm(img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad():\n",
    "        out = model(x)\n",
    "        prob = torch.softmax(out, dim=1).cpu().numpy().reshape(-1)\n",
    "    return prob\n",
    "\n",
    "# Build meta dataset from feat_df (posture) + CNN probabilities (appearance)\n",
    "META_N = min(600, len(feat_df))  # keep small for speed\n",
    "meta_rows = []\n",
    "\n",
    "for i in range(META_N):\n",
    "    r = feat_df.iloc[i]\n",
    "    prob = cnn_probs(r[\"path\"])\n",
    "    meta_rows.append({\n",
    "        \"species\": r[\"species\"],\n",
    "        \"crouch_score\": float(r[\"crouch_score\"]),\n",
    "        \"norm_h\": float(r[\"norm_h\"]),\n",
    "        \"aspect\": float(r[\"aspect\"]),\n",
    "        **{f\"p_{class_names[j]}\": float(prob[j]) for j in range(n_classes)}\n",
    "    })\n",
    "\n",
    "meta_df = pd.DataFrame(meta_rows)\n",
    "print(\"✅ meta_df created:\", meta_df.shape)\n",
    "\n",
    "# Stress-risk proxy: within each species, top 30% crouch_score -> \"stress_proxy\" = 1\n",
    "meta_df[\"stress_proxy\"] = 0\n",
    "for sp in meta_df[\"species\"].unique():\n",
    "    q = meta_df.loc[meta_df[\"species\"] == sp, \"crouch_score\"].quantile(0.70)\n",
    "    meta_df.loc[(meta_df[\"species\"] == sp) & (meta_df[\"crouch_score\"] >= q), \"stress_proxy\"] = 1\n",
    "\n",
    "# Feature matrices\n",
    "X_posture = meta_df[[\"crouch_score\",\"norm_h\",\"aspect\"]].to_numpy()\n",
    "X_species = meta_df[[c for c in meta_df.columns if c.startswith(\"p_\")]].to_numpy()\n",
    "X_fused   = np.concatenate([X_posture, X_species], axis=1)\n",
    "y = meta_df[\"stress_proxy\"].to_numpy()\n",
    "\n",
    "# Split indices (stratified)\n",
    "tr_idx, te_idx = train_test_split(\n",
    "    np.arange(len(y)), test_size=0.25, random_state=SEED, stratify=y\n",
    ")\n",
    "\n",
    "def train_eval(X, name):\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=400))\n",
    "    ])\n",
    "    clf.fit(X[tr_idx], y[tr_idx])\n",
    "    pred = clf.predict(X[te_idx])\n",
    "    proba = clf.predict_proba(X[te_idx])[:, 1]\n",
    "    return clf, pred, proba, {\n",
    "        \"Model\": name,\n",
    "        \"Accuracy\": float(accuracy_score(y[te_idx], pred)),\n",
    "        \"F1\": float(f1_score(y[te_idx], pred)),\n",
    "        \"ROC-AUC\": float(roc_auc_score(y[te_idx], proba)),\n",
    "    }\n",
    "\n",
    "clf_post, pred_post, proba_post, row_post = train_eval(X_posture, \"Posture-only\")\n",
    "clf_spec, pred_spec, proba_spec, row_spec = train_eval(X_species, \"Appearance-only (CNN probs)\")\n",
    "clf_fuse, pred_fuse, proba_fuse, row_fuse = train_eval(X_fused, \"Fused (posture + CNN probs)\")\n",
    "\n",
    "rq3_tab1 = pd.DataFrame([row_post, row_spec, row_fuse])\n",
    "rq3_tab1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a777297b-f008-4ac0-a0b4-893cdae34268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved RQ3_Fig1.pdf, RQ3_Fig2.pdf, RQ3_Tab1.xlsx, RQ3_Tab2.xlsx in: C:\\Users\\nitro\\Documents\\animals10_project\\outputs_part2_animals10\\Figures_Tables\\RQ3\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ3: Outputs (2 figures + 2 tables)\n",
    "# =========================\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# ---- Figure 1: ROC-AUC comparison ----\n",
    "fig = plt.figure(figsize=(7,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(rq3_tab1[\"Model\"], rq3_tab1[\"ROC-AUC\"])\n",
    "ax.set_ylim(0, 1)\n",
    "ax.set_ylabel(\"ROC-AUC\")\n",
    "ax.set_title(\"RQ3: Stress-risk proxy performance (unimodal vs fused)\")\n",
    "ax.tick_params(axis=\"x\", rotation=20)\n",
    "fig.savefig(FIG_ROOT/\"RQ3\"/\"RQ3_Fig1.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Figure 2: Confusion matrix for fused model ----\n",
    "cm3 = confusion_matrix(y[te_idx], pred_fuse)\n",
    "\n",
    "fig = plt.figure(figsize=(4.5,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.imshow(cm3)\n",
    "ax.set_title(\"RQ3: Confusion matrix (Fused model)\")\n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"True\")\n",
    "ax.set_xticks([0,1]); ax.set_yticks([0,1])\n",
    "ax.set_xticklabels([\"Not-stress\",\"Stress\"])\n",
    "ax.set_yticklabels([\"Not-stress\",\"Stress\"])\n",
    "for (i, j), v in np.ndenumerate(cm3):\n",
    "    ax.text(j, i, str(v), ha=\"center\", va=\"center\")\n",
    "fig.savefig(FIG_ROOT/\"RQ3\"/\"RQ3_Fig2.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Table 1: model comparison ----\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ3\"/\"RQ3_Tab1.xlsx\") as w:\n",
    "    rq3_tab1.to_excel(w, index=False, sheet_name=\"Model_Comparison\")\n",
    "\n",
    "# ---- Table 2: fused model coefficient importance ----\n",
    "feature_names = [\"crouch_score\",\"norm_h\",\"aspect\"] + [c for c in meta_df.columns if c.startswith(\"p_\")]\n",
    "coefs = clf_fuse.named_steps[\"lr\"].coef_.reshape(-1)\n",
    "\n",
    "rq3_tab2 = (pd.DataFrame({\n",
    "    \"feature\": feature_names,\n",
    "    \"coef\": coefs,\n",
    "    \"abs_coef\": np.abs(coefs)\n",
    "}).sort_values(\"abs_coef\", ascending=False).head(25))\n",
    "\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ3\"/\"RQ3_Tab2.xlsx\") as w:\n",
    "    rq3_tab2.to_excel(w, index=False, sheet_name=\"Top_Coefficients\")\n",
    "\n",
    "print(\"✅ Saved RQ3_Fig1.pdf, RQ3_Fig2.pdf, RQ3_Tab1.xlsx, RQ3_Tab2.xlsx in:\",\n",
    "      (FIG_ROOT/\"RQ3\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1280a26b-e545-4ad9-aa59-b34c41950365",
   "metadata": {},
   "source": [
    "## RQ4: Robustness under missing or noisy information\n",
    "\n",
    "Real-world welfare monitoring (e.g., cameras in farms/shelters/homes) often suffers from partial views, occlusions, and uncertainty. This section tests robustness of the fused stress-risk proxy system under:\n",
    "\n",
    "1) **Baseline fused** (all features available)  \n",
    "2) **Missing posture** (posture features unavailable)  \n",
    "3) **Low-confidence appearance** (CNN probabilities degraded when confidence is low)\n",
    "\n",
    "We report performance changes using Accuracy, F1, and ROC-AUC.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b0992070-bfb9-4e4f-aa2a-262ec6107c8e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Condition</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "      <th>ROC-AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Baseline fused</td>\n",
       "      <td>0.946903</td>\n",
       "      <td>0.926829</td>\n",
       "      <td>0.991781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Missing posture</td>\n",
       "      <td>0.646018</td>\n",
       "      <td>0.047619</td>\n",
       "      <td>0.552055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Low-confidence appearance (maxp&lt;0.4)</td>\n",
       "      <td>0.938053</td>\n",
       "      <td>0.915663</td>\n",
       "      <td>0.990411</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                              Condition  Accuracy        F1   ROC-AUC\n",
       "0                        Baseline fused  0.946903  0.926829  0.991781\n",
       "1                       Missing posture  0.646018  0.047619  0.552055\n",
       "2  Low-confidence appearance (maxp<0.4)  0.938053  0.915663  0.990411"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ4: Robustness experiments\n",
    "# =========================\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "\n",
    "def eval_condition(X, label):\n",
    "    clf = Pipeline([\n",
    "        (\"scaler\", StandardScaler()),\n",
    "        (\"lr\", LogisticRegression(max_iter=400))\n",
    "    ])\n",
    "    clf.fit(X[tr_idx], y[tr_idx])\n",
    "\n",
    "    pred = clf.predict(X[te_idx])\n",
    "    proba = clf.predict_proba(X[te_idx])[:, 1]\n",
    "\n",
    "    return {\n",
    "        \"Condition\": label,\n",
    "        \"Accuracy\": float(accuracy_score(y[te_idx], pred)),\n",
    "        \"F1\": float(f1_score(y[te_idx], pred)),\n",
    "        \"ROC-AUC\": float(roc_auc_score(y[te_idx], proba))\n",
    "    }, pred, proba, clf\n",
    "\n",
    "# 1) Baseline fused\n",
    "row_base, pred_base, proba_base, clf_base = eval_condition(X_fused, \"Baseline fused\")\n",
    "\n",
    "# 2) Missing posture: zero out crouch_score, norm_h, aspect (first 3 columns)\n",
    "X_missing_posture = X_fused.copy()\n",
    "X_missing_posture[:, :3] = 0\n",
    "row_miss_post, pred_miss_post, proba_miss_post, clf_miss_post = eval_condition(X_missing_posture, \"Missing posture\")\n",
    "\n",
    "# 3) Low-confidence appearance: if max class prob < threshold, zero out the prob vector\n",
    "X_lowconf = X_fused.copy()\n",
    "maxp = X_species.max(axis=1)\n",
    "THRESH = 0.40\n",
    "mask_low = maxp < THRESH\n",
    "X_lowconf[mask_low, 3:] = 0\n",
    "row_lowconf, pred_lowconf, proba_lowconf, clf_lowconf = eval_condition(X_lowconf, f\"Low-confidence appearance (maxp<{THRESH})\")\n",
    "\n",
    "rq4_tab1 = pd.DataFrame([row_base, row_miss_post, row_lowconf])\n",
    "rq4_tab1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec680bf5-eded-4b68-afe8-0ec85ba93596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved RQ4_Fig1.pdf, RQ4_Fig2.pdf, RQ4_Tab1.xlsx, RQ4_Tab2.xlsx in: C:\\Users\\nitro\\Documents\\animals10_project\\outputs_part2_animals10\\Figures_Tables\\RQ4\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ4: Outputs (2 figures + 2 tables)\n",
    "# =========================\n",
    "\n",
    "# ---- Figure 1: ROC-AUC by condition ----\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(rq4_tab1[\"Condition\"], rq4_tab1[\"ROC-AUC\"])\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel(\"ROC-AUC\")\n",
    "ax.set_title(\"RQ4: Robustness under missing/noisy inputs (ROC-AUC)\")\n",
    "ax.tick_params(axis=\"x\", rotation=20)\n",
    "fig.savefig(FIG_ROOT/\"RQ4\"/\"RQ4_Fig1.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Figure 2: F1 by condition ----\n",
    "fig = plt.figure(figsize=(8,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(rq4_tab1[\"Condition\"], rq4_tab1[\"F1\"])\n",
    "ax.set_ylim(0,1)\n",
    "ax.set_ylabel(\"F1 Score\")\n",
    "ax.set_title(\"RQ4: Robustness under missing/noisy inputs (F1)\")\n",
    "ax.tick_params(axis=\"x\", rotation=20)\n",
    "fig.savefig(FIG_ROOT/\"RQ4\"/\"RQ4_Fig2.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Table 1: Robustness performance ----\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ4\"/\"RQ4_Tab1.xlsx\") as w:\n",
    "    rq4_tab1.to_excel(w, index=False, sheet_name=\"Robustness\")\n",
    "\n",
    "# ---- Table 2: Delta vs baseline ----\n",
    "baseline = rq4_tab1.iloc[0].copy()\n",
    "rq4_tab2 = rq4_tab1.copy()\n",
    "rq4_tab2[\"Delta_Accuracy\"] = rq4_tab2[\"Accuracy\"] - float(baseline[\"Accuracy\"])\n",
    "rq4_tab2[\"Delta_F1\"]       = rq4_tab2[\"F1\"] - float(baseline[\"F1\"])\n",
    "rq4_tab2[\"Delta_ROC-AUC\"]  = rq4_tab2[\"ROC-AUC\"] - float(baseline[\"ROC-AUC\"])\n",
    "\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ4\"/\"RQ4_Tab2.xlsx\") as w:\n",
    "    rq4_tab2.to_excel(w, index=False, sheet_name=\"Delta_vs_Baseline\")\n",
    "\n",
    "print(\"✅ Saved RQ4_Fig1.pdf, RQ4_Fig2.pdf, RQ4_Tab1.xlsx, RQ4_Tab2.xlsx in:\",\n",
    "      (FIG_ROOT/\"RQ4\").resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f09bfb15-5654-4395-a7c7-b71500642a8d",
   "metadata": {},
   "source": [
    "## RQ5: Explainability and rule-based decision support for welfare screening\n",
    "\n",
    "For animal welfare applications, predictions should be interpretable and actionable for end users (e.g., farmers, veterinarians, and pet owners). This section adds a simple **rule engine** that converts the stress-risk proxy into human-readable levels (Low/Moderate/High) and flags uncertain cases for **human review**.\n",
    "\n",
    "The rule engine uses:\n",
    "- a posture-based threshold (within-species crouch-score percentile), and\n",
    "- a confidence heuristic based on CNN probability strength.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "56e7f497-16d0-4e4b-9e65-e994cd318e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Rule level counts:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rule_level</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>High</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Low</td>\n",
       "      <td>292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Moderate</td>\n",
       "      <td>137</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  rule_level  count\n",
       "0       High     21\n",
       "1        Low    292\n",
       "2   Moderate    137"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Example rows:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>species</th>\n",
       "      <th>crouch_score</th>\n",
       "      <th>stressed_proxy_flag</th>\n",
       "      <th>rule_level</th>\n",
       "      <th>human_review</th>\n",
       "      <th>top3_probs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>pecora</td>\n",
       "      <td>0.107143</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>[('cane', 0.5379108190536499), ('scoiattolo', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mucca</td>\n",
       "      <td>0.182927</td>\n",
       "      <td>1</td>\n",
       "      <td>High</td>\n",
       "      <td>1</td>\n",
       "      <td>[('gallina', 0.2859024107456207), ('farfalla',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pecora</td>\n",
       "      <td>0.133333</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>[('cane', 0.6948132514953613), ('pecora', 0.10...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ragno</td>\n",
       "      <td>0.116732</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>[('ragno', 0.6813032627105713), ('farfalla', 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>cavallo</td>\n",
       "      <td>0.080333</td>\n",
       "      <td>0</td>\n",
       "      <td>Low</td>\n",
       "      <td>0</td>\n",
       "      <td>[('cane', 0.6530382633209229), ('cavallo', 0.1...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   species  crouch_score  stressed_proxy_flag rule_level  human_review  \\\n",
       "0   pecora      0.107143                    0        Low             0   \n",
       "1    mucca      0.182927                    1       High             1   \n",
       "2   pecora      0.133333                    0        Low             0   \n",
       "3    ragno      0.116732                    0        Low             0   \n",
       "4  cavallo      0.080333                    0        Low             0   \n",
       "\n",
       "                                          top3_probs  \n",
       "0  [('cane', 0.5379108190536499), ('scoiattolo', ...  \n",
       "1  [('gallina', 0.2859024107456207), ('farfalla',...  \n",
       "2  [('cane', 0.6948132514953613), ('pecora', 0.10...  \n",
       "3  [('ragno', 0.6813032627105713), ('farfalla', 0...  \n",
       "4  [('cane', 0.6530382633209229), ('cavallo', 0.1...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ5: Rule engine + explainability tables\n",
    "# =========================\n",
    "\n",
    "# We assume meta_df exists from RQ3 and includes:\n",
    "# crouch_score, species, and p_* columns, plus stress_proxy\n",
    "\n",
    "prob_cols = [c for c in meta_df.columns if c.startswith(\"p_\")]\n",
    "if len(prob_cols) == 0:\n",
    "    raise RuntimeError(\"No probability columns found in meta_df. Rerun RQ3.\")\n",
    "\n",
    "# Threshold map: within each species, compute the 70th percentile of crouch_score\n",
    "q70_map = meta_df.groupby(\"species\")[\"crouch_score\"].quantile(0.70).to_dict()\n",
    "\n",
    "def topk_classes(prob_vec, k=3):\n",
    "    idx = np.argsort(prob_vec)[::-1][:k]\n",
    "    return [(class_names[i], float(prob_vec[i])) for i in idx]\n",
    "\n",
    "def rule_engine(species, crouch_score, max_prob):\n",
    "    \"\"\"\n",
    "    Simple interpretable rule layer:\n",
    "    - 'stressed_proxy' if crouch_score >= species-specific threshold\n",
    "    - 'human_review' if max_prob is low (uncertain classification)\n",
    "    - rule_level: Low / Moderate / High\n",
    "    \"\"\"\n",
    "    stressed_proxy = crouch_score >= q70_map.get(species, 1.0)\n",
    "    human_review = max_prob < 0.40\n",
    "\n",
    "    if stressed_proxy and human_review:\n",
    "        level = \"High\"\n",
    "    elif stressed_proxy:\n",
    "        level = \"Moderate\"\n",
    "    else:\n",
    "        level = \"Low\"\n",
    "\n",
    "    return level, int(human_review), int(stressed_proxy)\n",
    "\n",
    "# Apply rule engine to all rows\n",
    "levels, reviews, stressed_flags = [], [], []\n",
    "top3_list = []\n",
    "\n",
    "for i in range(len(meta_df)):\n",
    "    sp = meta_df.loc[i, \"species\"]\n",
    "    cs = float(meta_df.loc[i, \"crouch_score\"])\n",
    "    probs = meta_df.loc[i, prob_cols].to_numpy()\n",
    "    max_prob = float(probs.max())\n",
    "\n",
    "    lvl, rev, st = rule_engine(sp, cs, max_prob)\n",
    "    levels.append(lvl)\n",
    "    reviews.append(rev)\n",
    "    stressed_flags.append(st)\n",
    "    top3_list.append(str(topk_classes(probs, 3)))\n",
    "\n",
    "meta_df[\"rule_level\"] = levels\n",
    "meta_df[\"human_review\"] = reviews\n",
    "meta_df[\"stressed_proxy_flag\"] = stressed_flags\n",
    "meta_df[\"top3_probs\"] = top3_list\n",
    "\n",
    "# Summary tables\n",
    "rq5_tab1 = meta_df.groupby(\"rule_level\").size().reset_index(name=\"count\")\n",
    "rq5_tab2 = (pd.crosstab(meta_df[\"species\"], meta_df[\"rule_level\"], normalize=\"index\")\n",
    "            .reset_index())\n",
    "\n",
    "# Examples table (small)\n",
    "rq5_examples = meta_df[[\"species\",\"crouch_score\",\"stressed_proxy_flag\",\"rule_level\",\"human_review\",\"top3_probs\"]].head(20)\n",
    "\n",
    "print(\"✅ Rule level counts:\")\n",
    "display(rq5_tab1)\n",
    "\n",
    "print(\"✅ Example rows:\")\n",
    "display(rq5_examples.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "859af1c2-973b-4003-9bc8-3a009c9aa107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved RQ5_Fig1.pdf, RQ5_Fig2.pdf, RQ5_Tab1.xlsx, RQ5_Tab2.xlsx in: C:\\Users\\nitro\\Documents\\animals10_project\\outputs_part2_animals10\\Figures_Tables\\RQ5\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# RQ5: Outputs (2 figures + 2 tables)\n",
    "# =========================\n",
    "\n",
    "# ---- Figure 1: Rule-level distribution ----\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar(rq5_tab1[\"rule_level\"], rq5_tab1[\"count\"])\n",
    "ax.set_title(\"RQ5: Rule-based stress-risk level distribution (proxy)\")\n",
    "ax.set_xlabel(\"Rule level\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "fig.savefig(FIG_ROOT/\"RQ5\"/\"RQ5_Fig1.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Figure 2: Human review flags ----\n",
    "review_counts = meta_df[\"human_review\"].value_counts().sort_index()\n",
    "no_review = int(review_counts.get(0, 0))\n",
    "needs_review = int(review_counts.get(1, 0))\n",
    "\n",
    "fig = plt.figure(figsize=(6,4))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.bar([\"No review\", \"Needs review\"], [no_review, needs_review])\n",
    "ax.set_title(\"RQ5: Human-review flags (uncertainty handling)\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "fig.savefig(FIG_ROOT/\"RQ5\"/\"RQ5_Fig2.pdf\", bbox_inches=\"tight\")\n",
    "plt.close(fig)\n",
    "\n",
    "# ---- Table 1: Rule summary + examples + review rate ----\n",
    "review_rate = float(meta_df[\"human_review\"].mean())\n",
    "review_rate_df = pd.DataFrame([{\"human_review_rate\": review_rate}])\n",
    "\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ5\"/\"RQ5_Tab1.xlsx\") as w:\n",
    "    rq5_tab1.to_excel(w, index=False, sheet_name=\"RuleLevel_Counts\")\n",
    "    review_rate_df.to_excel(w, index=False, sheet_name=\"ReviewRate\")\n",
    "    rq5_examples.to_excel(w, index=False, sheet_name=\"Examples\")\n",
    "\n",
    "# ---- Table 2: Rule levels by species (row-normalized) ----\n",
    "with pd.ExcelWriter(FIG_ROOT/\"RQ5\"/\"RQ5_Tab2.xlsx\") as w:\n",
    "    rq5_tab2.to_excel(w, index=False, sheet_name=\"RuleLevel_BySpecies\")\n",
    "\n",
    "print(\"✅ Saved RQ5_Fig1.pdf, RQ5_Fig2.pdf, RQ5_Tab1.xlsx, RQ5_Tab2.xlsx in:\",\n",
    "      (FIG_ROOT/\"RQ5\").resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b003155-2422-4a67-aebd-5cf42d7e3ca7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ALL REQUIRED FILES EXIST\n",
      "Total files checked: 20\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Final Compliance Check\n",
    "# =========================\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "required = []\n",
    "for rq in range(1, 6):\n",
    "    folder = FIG_ROOT / f\"RQ{rq}\"\n",
    "    required.extend([\n",
    "        folder / f\"RQ{rq}_Fig1.pdf\",\n",
    "        folder / f\"RQ{rq}_Fig2.pdf\",\n",
    "        folder / f\"RQ{rq}_Tab1.xlsx\",\n",
    "        folder / f\"RQ{rq}_Tab2.xlsx\",\n",
    "    ])\n",
    "\n",
    "missing = [str(p) for p in required if not p.exists()]\n",
    "\n",
    "if missing:\n",
    "    print(\"❌ Missing required files:\")\n",
    "    for m in missing:\n",
    "        print(\" -\", m)\n",
    "else:\n",
    "    print(\"✅ ALL REQUIRED FILES EXIST\")\n",
    "    print(\"Total files checked:\", len(required))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c539bad-db87-4383-b95e-79e91792e5fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ZIP created at: C:\\Users\\nitro\\Documents\\animals10_project\\outputs_part2_animals10\\Figures_Tables.zip\n"
     ]
    }
   ],
   "source": [
    "# =========================\n",
    "# Create Figures_Tables ZIP\n",
    "# =========================\n",
    "\n",
    "import zipfile\n",
    "\n",
    "zip_path = BASE_DIR / \"Figures_Tables.zip\"\n",
    "\n",
    "with zipfile.ZipFile(zip_path, \"w\", zipfile.ZIP_DEFLATED) as z:\n",
    "    for rq_dir in FIG_ROOT.iterdir():\n",
    "        if rq_dir.is_dir():\n",
    "            for f in rq_dir.iterdir():\n",
    "                z.write(f, arcname=f\"Figures_Tables/{rq_dir.name}/{f.name}\")\n",
    "\n",
    "print(\"✅ ZIP created at:\", zip_path.resolve())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:animals10_pr]",
   "language": "python",
   "name": "conda-env-animals10_pr-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
